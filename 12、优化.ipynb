{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道在深度网络中，它是一个非线性函数的拟合，而其损失函数的优化，也就自然是一个非凸函数的优化问题，而对于非凸的优化问题，是一个NP-hard问题。但是非凸，并不等于就无法train了，我们还是可以寻找local minima的。\n",
    "\n",
    "对于非凸问题，我们如何去找这个local minima，或者是local maxima呢？也就是当梯度为0时，这个驻点它是极小值，极大值，还是即极大，又极小呢？\n",
    "\n",
    "大多数认为，当训练不下去时，就卡在了local minima，其实呢，在深度学习中较多的是鞍点，并不是local minima...\n",
    "\n",
    "一般求极值的方法，就是令梯度为0，然后将得到的值带入式子，找到极小的那个值，即为极小值，这个是数学上的方法，我们从另一个角度来看，任何函数可以使用泰勒公式展开，当一阶导为0时，这个时候我们假定$x \\to x^0$是无限接近的，就可以忽略二阶导后面的值（因为值真的很小），这个时候需要求二阶导，也就是我们通常所说的海森矩阵... 由二阶导引出的一个优化方法，就是牛顿法，牛顿法是考虑二阶导的，也就是不能忽略该值的作用的，牛顿法从物理角度来理解，它是求梯度的梯度，速度的梯度是加速度，加速度的梯度，也就是二阶导，它的目标优化方向更加直接，如果存在极值点，是一步到位，但是，这个方法很明显的缺点就是需要求海森矩阵，而且还要求其逆，也就是要求海森矩阵可逆，对于大数据集、大特征的情况，特别是在深度学习中，这种方法在现在这样的硬件条件下是不可行的... 或许将来等到量子计算的广泛应用，就可以更快了！\n",
    "\n",
    "得到海森矩阵后，我们该如何利用海森矩阵来判断呢？稍微懂得线性代数知识的都会知道，如果我们有一个矩阵$A$，则有$Ax = \\lambda x$，$\\lambda$则为该矩阵的特征值，而$x$则为该矩阵的特征向量，这里我们就需要求得该特征值$diag(\\lambda)$，如果该特征值都是正的，那么既有$x^THx = x^T \\lambda x = \\lambda \\|x\\|^2 > 0$，即该矩阵为正定矩阵，固然通过泰勒展开得到所有的可行解$f(\\theta) > f(\\theta^0)$的，即存在local minima，相反如果小于0，则存在local maxima，如果特征值中既有正值，又有负值，也就是我们经常所的鞍点，如果有等于0的情况，那就无法判断了，某个方向是鞍点，某个方向存在极小或极大，这个时候在高维空间中，很有可能卡住，逃不出去，这个时候可以给其一个扰动，也就是将该值赋予一个远离鞍点的值，使其能够往其它方向去走，也就可以继续往下训练。前面说了正定矩阵，还有负定矩阵，半正定，半负定，和不定矩阵，这些定义都是与$x^THx$有关的。有一个特征值全为0的矩阵，我们称之为Monkey鞍点，也是非常有意思...\n",
    "\n",
    "在深度线性网络中，所有的local minima，其实都是global minima。激活函数都是relu这样的深度非线性网络是存在local minima的。\n",
    "\n",
    "大部分情况，我们如果在local minima了，其实它的值与global minima也差不了多少。如果是在鞍点处，说明还有可优化的空间，这个时候的损失还是比较大的，而在local minima则说明损失比较小，所以一般认为的在鞍点会出不去，其实大部分情况都是可以训练的，且是能够进一步减少损失的。而且，我们在一定初始化条件，和数据样本的情况下，也是可以找到global minima的。\n",
    "\n",
    "随着网络参数的增加，几乎很难遇到local minima，或者是local maxima，几乎都是鞍点，所有在这个环境中，我们还是可以正常训练，所以也就无需担心会卡住的问题，因为local minima很少，如果到了local minima，说明与global minima也差不了多少，因为local minima很少，global minima就更少。（这边为什么说随着参数的增加，local minima和local maxima会很少，我们假设海森矩阵的特征值，有一半概率为正，一半概率为负，那么随着参数的增多，全为正，或全为负的概率为$(1/2)^n$，$n$为参数的个数，所以这个时候大部分都是鞍点！）\n",
    "\n",
    "如果网络足够大的话，我们是可以找到使用梯度下降找到全局最优的。\n",
    "\n",
    "即使我们选择不同的初始值、或者不同的策略（比如优化算法）最终的损失其实还是相似的，或者我们还可以使用混合算法。（如果说深度学习中的鞍点很多的话，在遇到鞍点时，我们选择一致往损失减少的那个方向走呢，而不是走一个更难优化的方向，就比如水由于重力作用，一直会往下走，我们按照这种思路，一直让其选择这种路径，也就构成了看似最佳的优化路径，而不去选择可能存在的捷径！如果按照这种思路，能否找到全局最优呢？）\n",
    "\n",
    "Batch Normalization的方法也从某种程度上帮助了更好地优化（比如跳过鞍点），和探索全局最优。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
