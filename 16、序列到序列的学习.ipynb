{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前也说到过，关于RNN的一些，如DRNN，BRNN，Naive RNN，LSTM，GRU等，这里就不再赘述。\n",
    "\n",
    "序列生成是由字符/单词组成，那我们的目标也就是生成这些基本组成元素。除了一般的文本生成，还有图像生成，另外考虑到图像相邻的影响，我们有PixelRNN。一般我们所说的生成，也都是有条件的生成，不然无法得到我们期望的...如看图说话，聊天机器人，机器翻译...\n",
    "\n",
    "对于生成向量的方式，我们有词嵌入，CNN，自编码器等方法，所以不是固定的... 对于聊天机器人，问答问题的应用，我们还需要引入注意力机制...\n",
    "\n",
    "一般机器翻译，在训练阶段都是把最终输出作为测试的辅助输入的，而对于动态条件生成的应用，可能不能直接将最终输出作为所有测试的输入，应该按照测试阶段的每阶段的需要进行按需输入，那么问题来了，该如何动态实现呢？\n",
    "\n",
    "注意力机制中，我们会将训练中的隐状态信息与测试阶段的隐含信息做match，来得到所有训练阶段可能关联的状态值，然后再经过softmax，得到总概率为1的分布，最终来个argmax得到测试阶段每次相关的辅助输入。\n",
    "\n",
    "还有图像聚焦，语音关键词聚焦，文本总结等都是这类应用...\n",
    "\n",
    "在序列生成中，会出现训练和测试mismatch的问题，训练如果选择真实值作为下次的输入，而测试集只能是预测值作为输入，训练阶段是按照正确的路径在走，而测试阶段如果有一步走错，那后面的还是会按照训练的正确路径去走，会有一步错的情况；而如果训练也选择预测值，可能会得到步步错的情况，因为训练使用预测值是无法保证正确性的。对于这样的情况，最直观的解决方案，就是随机选择使用真实值还是预测值，比如使用抛硬币的方法来选择；使用beam search方法，类似一颗分叉树，搜索可能的beam size大小的路径，选择其中最优的那条作为最终结果，类似语言模型中的log概率最大值；还有一种想法就是使用强化学习来让机器自己选择最佳的结果作为输入，也就是这个action集即为选择目标。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
