{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的简史：\n",
    "\n",
    "* 1958: 线性感知机\n",
    "* 1969: 感知机的局限性\n",
    "* 1980s: 多层感知机\n",
    "    * 对今天的DNN并没有什么两样\n",
    "* 1986: 反向传播\n",
    "    * 一般二元，超过3层隐层的网络就很难train了\n",
    "* 1989: 一层隐层就足够了，为什么还需要深度神经网络\n",
    "* 2006： RBM初始化打破僵局\n",
    "* 2009: GPU的应用\n",
    "* 2011: 开始在语音识别上流行\n",
    "* 2012: 赢得ILSVRC图像竞赛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的三个步骤：\n",
    "\n",
    "1. 构建一个神经网络\n",
    "2. 找到fitness函数\n",
    "3. 优化，找到最佳函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全连接前馈神经网络：\n",
    "\n",
    "输入层、隐层和输出层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度神经网络的演变：\n",
    "\n",
    "AlexNet(2012)、VGG(2014)、GoogleNet(2014)、ResidualNet(2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的出现是从传统机器学习的特征工程到通过机器自动抽取特征的过度，但实际问题变成了如何架构好的网络架构能够抽取到合理的隐性特征，并得到好的结果。\n",
    "\n",
    "即使同一个网络架构，由于其存在的参数由千千万万个，也就是可能会有n多个function set，我们需要在这个n个function set尽量找出最好的function来拟合数据。\n",
    "\n",
    "深度学习的目标也是要找到预测的结果要符合真实值，也就是无论对于回归问题，还是分类问题，都是使得预测与真实值之间的损失最小的参数值的搜索过程。一般对于分类问题，都是使用交叉熵损失。\n",
    "\n",
    "深度学习的训练，一般都是使用的梯度下降算法..\n",
    "\n",
    "常见的BP计算的工具有：Tensorflow、torch、theano、caffe、CNTK、Chainer、Caffe、CNTK、DSSTNE、MXNET、libdnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，层数越深，参数越多，模型越复杂，训练得到的精度也就更高。\n",
    "\n",
    "对于任意连续函数，我们使用单层隐层的浅层神经网络，也可以训练的很好，只要神经元的个数足够多。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
